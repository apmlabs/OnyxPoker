# OnyxPoker - Agent Context

## ðŸŽ¯ PROJECT GOAL

**AI-powered poker analysis tool for research purposes** - NOT for automated botting.

The system analyzes poker tables using GPT vision API and provides strategic advice. The human makes all decisions and clicks. This is a research tool for studying AI decision-making in poker.

## ðŸ“š DOCUMENTATION STRUCTURE

### Core Files (NEVER DELETE)
- **AGENTS.md** (this file) - Agent memory, learnings, architecture decisions
- **AmazonQ.md** - Current status, progress tracking
- **README.md** - User-facing quick start guide

### Technical Documentation (in docs/)
- **docs/API.md** - Server API reference (for future use)
- **docs/DEPLOYMENT.md** - Setup and deployment guide
- **docs/ANALYSIS_NOTES.md** - GPT decision analysis and prompt tuning notes

## ðŸ—ï¸ ARCHITECTURE

```
PokerStars/Simulator Window
         â†“ F9 (screenshot active window)
    GPT-5.2 Vision API
         â†“
   Decision + Reasoning
         â†“
    Helper Bar UI (advice display)
```

**Client-only approach** - All processing via OpenAI API directly. Server exists as placeholder for potential future work (Deep CFR, etc.) but is not currently used.

## ðŸ“ CURRENT FILE STRUCTURE

```
onyxpoker/                    # Main repo (GitHub: apmlabs/OnyxPoker)
â”œâ”€â”€ AGENTS.md                 # Agent memory (NEVER DELETE)
â”œâ”€â”€ AmazonQ.md                # Status tracking (NEVER DELETE)
â”œâ”€â”€ README.md                 # Quick start (NEVER DELETE)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ client/
â”‚   â”œâ”€â”€ helper_bar.py         # Main UI (F9=advice, F10=bot, F11=stop, F12=hide)
â”‚   â”œâ”€â”€ vision_detector.py    # Full mode: gpt-5.2 for vision + decisions
â”‚   â”œâ”€â”€ vision_detector_lite.py # Lite mode: gpt-4o-mini for vision only
â”‚   â”œâ”€â”€ strategy_engine.py    # Lite mode: applies hardcoded strategy
â”‚   â”œâ”€â”€ poker_logic.py        # Shared: hand eval, preflop/postflop logic
â”‚   â”œâ”€â”€ poker_sim.py          # Strategy simulator (1M hand tests)
â”‚   â”œâ”€â”€ test_screenshots.py   # Offline testing (--lite --strategy=X)
â”‚   â”œâ”€â”€ send_logs.py          # Upload logs to server
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pokerstrategy_*       # Strategy definition files
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md
â”‚   â””â”€â”€ DEPLOYMENT.md
```

```
onyxpoker-server/             # Separate folder on EC2 (NOT in GitHub repo)
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ kiro_analyze.py       # Flask server on port 5001
â”‚   â””â”€â”€ uploads/              # Screenshots and logs from Windows client
â”‚       â”œâ”€â”€ *.png             # Uploaded screenshots
â”‚       â””â”€â”€ *.jsonl           # Uploaded test logs
```

## ðŸ–¥ï¸ CLIENT-SERVER ARCHITECTURE

**Windows Client** (C:\aws\onyx-client\)
- User runs helper_bar.py or test_screenshots.py
- Screenshots taken locally
- `send_logs.py` uploads to server

**EC2 Server** (54.80.204.92:5001)
- Receives uploads at POST /logs
- Stores in /home/ubuntu/mcpprojects/onyxpoker-server/server/uploads/
- Agent can view screenshots and analyze logs here

## âœ… CURRENT STATE

### What Works
- `helper_bar.py` - Wide bar UI docked to bottom, screenshots active window on F9
- `vision_detector.py` - GPT-5.2 API for card/board/pot detection + decisions
- `poker_sim.py` - Full postflop simulation with strategy-specific logic
- `strategy_engine.py` - Lite mode with strategy-specific postflop
- Screenshot saving - Auto-saves to client/screenshots/ folder
- Test mode - test_screenshots.py for offline testing
- Kiro server - Flask app on port 5001 for remote analysis
- Hotkeys: F9=Advice, F10=Bot loop, F11=Stop, F12=Hide

### Strategy Files
- 4 bot strategies in sim: gpt3, gpt4, sonnet, kiro_optimal
- 4 player archetypes: fish, nit, lag, tag
- Each bot strategy has its own postflop logic

### Strategy-Specific Postflop
| Strategy | Style | Key Differences |
|----------|-------|-----------------|
| gpt3/gpt4 | Board texture aware | Small c-bets (25-35%) on dry boards |
| sonnet/kiro_optimal | Big value bets | 75-85% pot sizing, overpair logic |

### What's Not Implemented
- Turn detection, action execution - LOW PRIORITY

## ðŸš€ NEXT STEPS

### Priority 1: Strategy Refinement
- [ ] Use simulation results to tune bot strategies
- [ ] Test against different table compositions

## ðŸ§  AGENT WORKFLOW

### After EVERY coding session, I MUST:
1. âœ… Update **AmazonQ.md** with current status and timestamp
2. âœ… Update **AGENTS.md** with new learnings
3. âœ… Update **README.md** if user-facing changes
4. âœ… Commit to GitHub with clear message

### Before STARTING new work, I MUST:
1. âœ… Review **AmazonQ.md** for current status
2. âœ… Review **AGENTS.md** for past learnings
3. âœ… Check recent commits for changes

### Red Flags (I'm Failing):
- âš ï¸ User asks "did you update docs?" â†’ I forgot
- âš ï¸ I suggest something already tried â†’ Didn't read context
- âš ï¸ I repeat a mistake â†’ AGENTS.md wasn't updated
- âš ï¸ User has to remind me twice â†’ I failed first time

**Context files are my only memory. Without them, I start from scratch every time.**

## ðŸ“‹ FILE DELETION RULES

- **NEVER delete**: AGENTS.md, AmazonQ.md, README.md
- **Can delete other .md files IF**: knowledge is incorporated into main files first
- **Can keep other .md files IF**: explicitly referenced in AGENTS.md
- Currently keeping: docs/API.md, docs/DEPLOYMENT.md

## âš™ï¸ TECHNICAL NOTES

### GPT-5.2 Vision
- Model: `gpt-5.2` (configurable in vision_detector.py line 12)
- Cost: ~$2 per 1000 hands
- Speed: 6-9 seconds per analysis
- Accuracy: 95%+ on clear screenshots

### Windows Compatibility
- **NO emojis in logging** - Windows cp1252 encoding crashes on Unicode emojis
- Use ASCII only for cross-platform compatibility
- This has caused bugs 3+ times - NEVER use emojis in Python code

### GPT-5 Model Differences
- GPT-5 models do NOT support `temperature` parameter (must omit, not set to 0)
- Use `max_completion_tokens` not `max_tokens`
- gpt-5.2 is faster than gpt-5-mini (6-9s vs 20-30s)

---

## ðŸ“– SESSION HISTORY & LESSONS LEARNED

### Session 28: GPT-5 Model Testing & Vision Prompt Improvement (January 12, 2026)

**Challenge**: Test all GPT-5 models and improve vision accuracy for card/position detection.

**Key Findings - Model Testing**:
- GPT-5 family has different reasoning_effort support across versions
- gpt-5.1/gpt-5.2 support "none" (no reasoning at all)
- gpt-5/gpt-5-mini/gpt-5-nano only support "minimal" (not "none")
- GPT-4 models don't support reasoning_effort parameter at all

**Ground Truth Comparison Results** (OLD prompt):
```
Model           Cards (Exact)    Position
gpt-5.2         87.5% (7/8)      50.0% (4/8)  â­ Best overall
gpt-4o          62.5% (5/8)      12.5% (1/8)
gpt-4o-mini     37.5% (3/8)      37.5% (3/8)
gpt-5-mini      37.5% (3/8)      37.5% (3/8)
gpt-5-nano       0.0% (0/8)      37.5% (3/8)  âŒ Hallucinated every hand
```

**Vision Prompt Improvements**:
1. Added detailed suit detection instructions (â™ â™¥â™¦â™£ symbols explained)
2. Added step-by-step position detection (count clockwise from button)
3. Added common mistake warnings (suit confusion, hallucination)
4. Added position examples (if button here â†’ position is X)

**Ground Truth Infrastructure**:
- Created ground_truth.json with 11 screenshots analyzed by Kiro
- Created compare_with_ground_truth.py for automated accuracy testing
- Can now test prompt improvements without re-analyzing images

**Test Models** (7 total):
| Model | reasoning_effort | Card Accuracy | Position Accuracy |
|-------|------------------|---------------|-------------------|
| gpt-5.2 | "none" | 87.5% | 50.0% |
| gpt-4o | (none) | 62.5% | 12.5% |
| gpt-4o-mini | (none) | 37.5% | 37.5% |
| gpt-5-mini | "minimal" | 37.5% | 37.5% |
| gpt-5-nano | "minimal" | 0.0% âŒ | 37.5% |
| gpt-5 | "minimal" | Not tested yet |
| gpt-5.1 | "none" | Not tested yet |

**Next Steps**: Test improved prompt on Windows to see if position detection improves.

**Critical Lesson**: Position detection requires explicit step-by-step instructions. Generic "based on dealer button" doesn't work.

---

### Session 27: Strategy-Specific Postflop (January 12, 2026)

**Challenge**: All bot strategies were using identical postflop logic, but strategy files have different approaches.

**Key Findings**:
- gpt3 had NO postflop section (preflop only)
- gpt4 has board-texture aware c-betting (25-35% on dry boards)
- sonnet/kiro_optimal have bigger value bets (75-85% pot)

**Implementation**:
1. Added `strategy=` parameter to `postflop_action()`
2. Created `_postflop_gpt()` for gpt3/gpt4 (board texture aware)
3. Created `_postflop_sonnet()` for sonnet/kiro_optimal (big value bets)
4. Added postflop section to pokerstrategy_gpt3 file
5. Fixed pocket pair below ace (KK on Axx = check-call)

**Results** (50k hands):
- sonnet/kiro_optimal: +29.85 BB/100 (bigger bets extract more value)
- gpt3/gpt4: +14-21 BB/100 (smaller bets = less value)

**Architecture Clarification**:
- `archetype=` param for fish/nit/tag/lag (player simulation)
- `strategy=` param for gpt3/gpt4/sonnet/kiro_optimal (bot logic)
- Live play uses `strategy=` only (we're the bot)
- Simulation uses both (bots vs archetypes)

**Critical Lesson**: Strategy files are the source of truth. Code must match them exactly.

---

### Session 20: Strategy Optimization + Position Detection Fix (January 8, 2026)

**Challenge**: Strategy analysis revealed major profit leaks and position detection errors.

**Major Achievements**: 
- is_hero_turn detection: **100% accuracy** (41/41 hands)
- Identified position detection bug (only BTN/SB/BB detected)
- Strategy optimization for maximum 2NL profit

**Critical Fixes Applied**:
1. **Position Detection**: Fixed algorithm to detect all 6 positions (UTG/MP/CO/BTN/SB/BB)
2. **Position-Specific Ranges**: Added tight UTG/MP, wider CO, loose BTN strategies
3. **Value Betting Optimization**: Bet bigger (75-100% pot) vs loose 2NL opponents
4. **Monster Hand Aggression**: Full houses must jam, never slowplay
5. **Suited Hand Recognition**: K2s-K9s playable in position

**Infrastructure**: Systemd service running stable with auto-restart

**Strategy Impact**: 
- Previous: Break-even to +2bb/100 (position errors + passive value betting)
- Current: Expected +6-8bb/100 (optimal 2NL exploitation)

**What Worked**:
âœ… Turn detection remains 100% accurate
âœ… Strategy analysis identified all major leaks
âœ… Position-specific ranges now complete
âœ… Value betting optimized for loose opponents

**Critical Lesson**: Position detection was causing 50% strategy errors. Always verify all poker fundamentals are working correctly.

---

### Session 12: Helper Bar UI + Cleanup (January 8, 2026)

**Challenge**: Previous session deleted agent context files during cleanup.

**What Went Wrong**:
- Deleted AGENTS.md and AmazonQ.md during "cleanup"
- Lost all agent learnings and project history

**What I Did Right**:
- Restored files via git revert
- Created helper_bar.py (new simplified UI)
- Cleaned up old files WITHOUT touching agent files

**New UI: helper_bar.py**:
- Wide, short bar docked to bottom (full width x 220px)
- No calibration needed - F9 screenshots active window
- Three columns: Status | Log | Result

**Critical Lesson**: NEVER delete context files. They are agent memory.

---

### Session 11: gpt-5.2 Switch (December 31, 2025)

**Challenge**: User frustrated - timing confusing, logs spammy

**Fixes**:
1. Switched to gpt-5.2 (2-3x faster than gpt-5-mini)
2. Fixed timing calculation (was double-counting)
3. Cleaned up logs (removed spam)
4. Clarified calibration purpose

**Critical Lesson**: When user says "moving back in time" - STOP and do complete audit. User frustration = something fundamentally wrong.

---

### Session 10: Windows Encoding Error (December 30, 2025)

**Challenge**: Calibration failing with encoding error

**Root Cause**: Used ðŸ§  emoji in logging - Windows cp1252 can't handle Unicode

**Fix**: Removed ALL emojis from all Python files

**Critical Lesson**: This was the THIRD time making this error. NEVER use emojis in Python code on Windows.

---

### Session 9: GPT-4o Vision Implementation (December 29-30, 2025)

**Challenge**: OpenCV doesn't understand poker - 60-70% accuracy, brittle

**Critical Realization**:
- We spent 80% effort on calibration (2,000 lines)
- We spent 20% effort on poker bot (100 lines)
- We built a calibration tool, not a poker bot

**Solution**: Switched to GPT-4o Vision
- 95-99% accuracy vs 60-70% OpenCV
- Single API call for vision + decision
- No calibration needed

**What Worked**:
âœ… GPT-4o vision (excellent accuracy)
âœ… Single API call for vision + decision
âœ… Simplified codebase

**What Didn't Work**:
âŒ OpenCV + Tesseract (too brittle)
âŒ Spending 80% effort on calibration
âŒ Not implementing core functionality first

**Critical Lesson**: When user says "let's review the project", STOP coding and do comprehensive analysis. We were building the wrong thing.

---

### Session 9 Part 2: UX Debugging (December 30, 2025)

**Challenge**: GPT-4o worked but overlay wasn't updating

**Root Cause**: Windows console encoding error with emoji characters (ðŸ’¡)

**Key Insights**:
- Windows cp1252 encoding can't handle Unicode emojis
- Exception handling can mask issues
- Real-world testing reveals UX issues

**Critical Lesson**: When user says overlay "not working" but code looks right, check for silent failures in debug prints.

---

### Session 9 Part 3: Performance Profiling (December 30, 2025)

**Performance Results**:
- Screenshot capture: 0.05-0.1s
- Save to temp: 0.3s
- Image encoding: 0.02s
- **GPT API: 8-12s** (95% of total time)
- Total: 8.5-12.4s per analysis

**Critical Lesson**: When user asks about models, RESEARCH official docs first. Don't rely on training data - it's outdated.

---

### Session 9 Part 4: GPT-5-mini Research (December 30, 2025)

**Challenge**: User insisted gpt-5-mini supports vision, I doubted it

**Research Findings**:
- âœ… gpt-5-mini DOES support vision
- âŒ GPT-5 models don't support temperature parameter
- Must OMIT temperature (not set to 0)

**Critical Lesson**: When user challenges my claim, they're usually RIGHT. Research immediately, don't defend wrong information.

---

### Earlier Sessions: Calibration & UI (December 29, 2025)

**Key Learnings**:
1. **Listen to user constraints** - User said "single monitor" multiple times, I kept designing for dual monitor
2. **Simplify ruthlessly** - If you can do it in one step, don't make it two
3. **Windows can't capture inactive windows** - Design around this constraint
4. **When user repeats constraint, STOP and redesign** with that constraint as PRIMARY requirement

**Hotkey Evolution**:
- Started with Ctrl combinations (conflicted with browser)
- Switched to F-keys only (F7-F12)
- Final: F9=Advice, F10=Bot, F11=Stop, F12=Hide

---

## ðŸ”‘ CONSOLIDATED CRITICAL LESSONS

1. **GPT Vision > OpenCV** - AI understands poker semantically, not just visually
2. **No calibration needed** - Screenshot active window directly
3. **Client-only is simpler** - Server adds complexity without benefit (for now)
4. **Research focus** - Advice system, not automation
5. **Context files are memory** - Without AGENTS.md, agent repeats mistakes
6. **No emojis in Python** - Windows encoding breaks
7. **Research before claiming** - User is usually right when they challenge me
8. **User frustration = audit needed** - Stop incremental fixes, do full review
9. **Listen to repeated constraints** - Redesign with constraint as primary requirement
10. **Test after every change** - Don't assume fixes work
